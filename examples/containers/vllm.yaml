kind: Container
metadata:
  name: vllm-qwen
  namespace: test
  labels:
    type: inference
platform: runpod
accelerators:
  - "1:L4"
volumes:
  - source: /output
    dest: s3://nebulous-rs/test/trl-test-small
    driver: RCLONE_SYNC
    continuous: true
image: "vllm/vllm-openai:latest"
# TODO: we need to fix the entrypoint for runpod
command: |
  python -m vllm.serve \
    --model $MODEL \
    --port 8000 \
    --host 0.0.0.0 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 2048 \
    --max-num-seqs 2
env:
  - key: MODEL
    value: Qwen/Qwen2.5-3B-Instruct
  - key: HUGGING_FACE_HUB_TOKEN
    secret_name: huggingface-hub-token
restart: Never